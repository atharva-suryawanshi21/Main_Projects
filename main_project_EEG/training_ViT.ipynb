{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './eeg-during-mental-arithmetic-tasks-1.0.0/'\n",
    "\n",
    "rest_filepaths = []\n",
    "task_filepaths = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    if filename.endswith('.edf'):\n",
    "        label = filename.split('_')[-1].split('.')[0]\n",
    "\n",
    "        if label == '1':\n",
    "            rest_filepaths.append(filepath)\n",
    "        else:\n",
    "            task_filepaths.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example function to read and process data\n",
    "def process_data(filepath):\n",
    "    data = mne.io.read_raw_edf(filepath, preload=True)\n",
    "    data.set_eeg_reference()\n",
    "    data.filter(l_freq=0.5, h_freq=45)\n",
    "    \n",
    "    min_t, max_t = 0, 61.99\n",
    "    data.crop(tmin=min_t, tmax=max_t)\n",
    "    \n",
    "    tmin, tmax = 0, 1.0  # Epoch duration of 1 second\n",
    "    epochs = mne.make_fixed_length_epochs(data, duration=tmax, overlap = 0.5, preload=True)\n",
    "    \n",
    "    return epochs\n",
    "\n",
    "# Function to process all 'task' labeled files\n",
    "def process_task_files(filepaths):\n",
    "    epochs_data = []  # List to store epoch data\n",
    "    labels = []  # List to store corresponding labels\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        epochs = process_data(filepath)\n",
    "        epochs_data.extend(epochs.get_data())\n",
    "        labels.extend([1] * len(epochs))  # Assign label 1 for 'task' (assuming 'task' label)\n",
    "    \n",
    "    return np.array(epochs_data), np.array(labels)\n",
    "\n",
    "def process_rest_files(filepaths):\n",
    "    epochs_data = []  # List to store epoch data\n",
    "    labels = []  # List to store corresponding labels\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        epochs = process_data(filepath)\n",
    "        epochs_data.extend(epochs.get_data())\n",
    "        labels.extend([0] * len(epochs))  # Assign label 1 for 'task' (assuming 'task' label)\n",
    "    \n",
    "    return np.array(epochs_data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "rest_epochs_data, rest_labels = process_rest_files(rest_filepaths)\n",
    "\n",
    "task_epochs_data, task_labels = process_task_files(task_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4392, 21, 500), (4392, 21, 500))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_epochs_data.shape, task_epochs_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4392,), (4392,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_labels.shape , task_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_data_combined = np.concatenate([rest_epochs_data, task_epochs_data], axis=0)\n",
    "\n",
    "# Concatenate labels\n",
    "labels_combined = np.concatenate([rest_labels, task_labels], axis=0)\n",
    "\n",
    "# Shuffle (optional)\n",
    "# Use the same random seed for synchronizing shuffle across data and labels\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "shuffle_indices = np.random.permutation(len(labels_combined))\n",
    "data = epochs_data_combined[shuffle_indices]\n",
    "label = labels_combined[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs shape: (8784, 21, 500)\n",
      "Labels shape: (8784,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Epochs shape:\", data.shape)\n",
    "print(\"Labels shape:\", label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.load('data.npy')\n",
    "# label = np.load('label.npy')\n",
    "\n",
    "# # # Convert numpy arrays to PyTorch tensors\n",
    "# # data = torch.tensor(data, dtype=torch.float32)\n",
    "# # label = torch.tensor(label, dtype=torch.long)  # Assuming labels are integers (dtype=torch.long)\n",
    "\n",
    "# # Print shapes to verify\n",
    "# print(\"Epochs shape:\", data.shape)\n",
    "# print(\"Labels shape:\", label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_train: torch.Size([7027, 1, 21, 500])\n",
      "Size of X_test: torch.Size([1757, 1, 21, 500])\n",
      "Size of y_train: torch.Size([7027])\n",
      "Size of y_test: torch.Size([1757])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import  TensorDataset\n",
    "\n",
    "# Choosing Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# # Normalizing Labels to [0, 1, 2, 3]\n",
    "# y = labels - np.min(labels)\n",
    "y = label\n",
    "\n",
    "# Normalizing Input features: z-score(mean=0, std=1)\n",
    "X = (data - np.mean(data)) / np.std(data)\n",
    "\n",
    "# Checking the existance of null & inf in the dataset\n",
    "if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n",
    "    raise ValueError(\"Data contains NaNs or infinities after normalization.\")\n",
    "if np.any(np.isnan(y)) or np.any(np.isinf(y)):\n",
    "    raise ValueError(\"Labels contain NaNs or infinities.\")\n",
    "\n",
    "# Making the X,y tensors for K-Fold Cross Validation\n",
    "X_tensor = torch.Tensor(X).unsqueeze(1)\n",
    "y_tensor = torch.LongTensor(y)\n",
    "\n",
    "# Spliting  Data: 80% for Train and 20% for Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Converting to Tensor\n",
    "X_train = torch.Tensor(X_train).unsqueeze(1).to(device)\n",
    "X_test = torch.Tensor(X_test).unsqueeze(1).to(device)\n",
    "y_train = torch.LongTensor(y_train).to(device)\n",
    "y_test = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "# Creating Tensor Dataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Printing the sizes\n",
    "print(\"Size of X_train:\", X_train.size())\n",
    "print(\"Size of X_test:\", X_test.size())\n",
    "print(\"Size of y_train:\", y_train.size())\n",
    "print(\"Size of y_test:\", y_test.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"Split image into patches and then embed them.\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int, Size of the image (it is a square).\n",
    "\n",
    "    patch_size : int, Size of the patch (it is a square).\n",
    "\n",
    "    in_chans : int, Number of input channels.\n",
    "\n",
    "    embed_dim : int, The emmbedding dimension.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_patches : int, Number of patches inside of our image.\n",
    "\n",
    "    proj : nn.Conv2d, Convolutional layer that does both the splitting into patches\n",
    "        and their embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=(21,500), patch_size = (1,16),  in_chans = 1, embed_dim = 768):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_patches = (img_size[0]//patch_size[0]) * (img_size[1]//patch_size[1])\n",
    "        self.proj = nn.Conv2d(in_chans,\n",
    "                              embed_dim,\n",
    "                              kernel_size=patch_size,\n",
    "                              stride = (patch_size[0], patch_size[1]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x= x.flatten(2)\n",
    "        x=x.transpose(1,2)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int The input and out dimension of per token features.\n",
    "\n",
    "    n_heads : int Number of attention heads.\n",
    "\n",
    "    qkv_bias : bool If True then we include bias to the query, key and value projections.\n",
    "\n",
    "    attn_p : float Dropout probability applied to the query, key and value tensors.\n",
    "\n",
    "    proj_p : float Dropout probability applied to the output tensor.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    scale : float Normalizing constant for the dot product.\n",
    "\n",
    "    qkv : nn.Linear Linear projection for the query, key and value.\n",
    "\n",
    "    proj : nn.Linear Linear mapping that takes in the concatenated output of all attention\n",
    "        heads and maps it into a new space.\n",
    "\n",
    "    attn_drop, proj_drop : nn.Dropout Dropout layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads = 12, qkv_bias= True, attn_p=0., proj_p= 0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "        \n",
    "        if dim != self.dim:\n",
    "            raise ValueError(f'Input dim {dim} is not equal to the model dim {self.dim}')\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(n_samples, n_tokens, 3, self.n_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2,0,3,1,4)\n",
    "        \n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        k_t = k.transpose(-2,-1)\n",
    "        dp = (q @ k_t)*self.scale\n",
    "        attn = dp.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        weighted_avg = attn @ v \n",
    "        weighted_avg = weighted_avg.transpose(1,2)\n",
    "        weighted_avg = weighted_avg.flatten(2)\n",
    "        \n",
    "        x=self.proj(weighted_avg)\n",
    "        x= self.proj_drop(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multilayer perceptron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int Number of input features.\n",
    "\n",
    "    hidden_features : int Number of nodes in the hidden layer.\n",
    "\n",
    "    out_features : int Number of output features.\n",
    "\n",
    "    p : float Dropout probability.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    fc : nn.Linear The First linear layer.\n",
    "\n",
    "    act : nn.GELU GELU activation function.\n",
    "\n",
    "    fc2 : nn.Linear The second linear layer.\n",
    "\n",
    "    drop : nn.Dropout Dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features,out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int Embeddinig dimension.\n",
    "\n",
    "    n_heads : int Number of attention heads.\n",
    "\n",
    "    mlp_ratio : float Determines the hidden dimension size of the `MLP` module with respect\n",
    "        to `dim`.\n",
    "\n",
    "    qkv_bias : bool If True then we include bias to the query, key and value projections.\n",
    "\n",
    "    p, attn_p : float Dropout probability.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    norm1, norm2 : LayerNorm Layer normalization.\n",
    "\n",
    "    attn : Attention Attention module.\n",
    "\n",
    "    mlp : MLP MLP module.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads, mlp_ratio = 4.0, qkv_bias = True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6) \n",
    "        self.attn = Attention(dim,\n",
    "                              n_heads = n_heads,\n",
    "                              qkv_bias=qkv_bias,\n",
    "                              attn_p=attn_p,\n",
    "                              proj_p=p\n",
    "                              )\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6) \n",
    "        hidden_features = int(dim*mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_features = hidden_features,\n",
    "            out_features=dim\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x+ self.mlp(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Simplified implementation of the Vision transformer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int Both height and the width of the image (it is a square).\n",
    "\n",
    "    patch_size : int oth height and the width of the patch (it is a square).\n",
    "\n",
    "    in_chans : int Number of input channels.\n",
    "\n",
    "    n_classes : int Number of classes.\n",
    "\n",
    "    embed_dim : int Dimensionality of the token/patch embeddings.\n",
    "\n",
    "    depth : int Number of blocks.\n",
    "\n",
    "    n_heads : int Number of attention heads.\n",
    "\n",
    "    mlp_ratio : float Determines the hidden dimension of the `MLP` module.\n",
    "\n",
    "    qkv_bias : bool If True then we include bias to the query, key and value projections.\n",
    "\n",
    "    p, attn_p : float Dropout probability.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patch_embed : PatchEmbed Instance of `PatchEmbed` layer.\n",
    "\n",
    "    cls_token : nn.Parameter Learnable parameter that will represent the first token in the sequence.\n",
    "        It has `embed_dim` elements.\n",
    "\n",
    "    pos_emb : nn.Parameter Positional embedding of the cls token + all the patches.\n",
    "        It has `(n_patches + 1) * embed_dim` elements.\n",
    "\n",
    "    pos_drop : nn.Dropout Dropout layer.\n",
    "\n",
    "    blocks : nn.ModuleList List of `Block` modules.\n",
    "\n",
    "    norm : nn.LayerNorm Layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=(21,500),\n",
    "        patch_size = (1,16),\n",
    "        in_chans = 1,\n",
    "        n_classes = 2,\n",
    "        embed_dim = 768,\n",
    "        depth = 12,\n",
    "        n_heads = 12,\n",
    "        mlp_ratio = 4,\n",
    "        qkv_bias = True,\n",
    "        p = 0.0,\n",
    "        attn_p = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbeddings(\n",
    "            img_size=img_size,\n",
    "            patch_size = patch_size, \n",
    "            in_chans = in_chans, \n",
    "            embed_dim = embed_dim\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1,1+self.patch_embed.n_patches,embed_dim))      \n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "        \n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim = embed_dim,\n",
    "                    n_heads = n_heads,\n",
    "                    mlp_ratio = mlp_ratio,\n",
    "                    qkv_bias = qkv_bias,\n",
    "                    p = p,\n",
    "                    attn_p = attn_p,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim, eps = 1e-6)\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        cls_token = self.cls_token.expand(n_samples, -1,-1)\n",
    "        x = torch.cat((cls_token,x), dim=1)\n",
    "        x= x+ self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        cls_token_final = x[:,0] # only cls token\n",
    "        x = self.head(cls_token_final)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 810.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 335.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# print(epoch , i)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mViT_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     39\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Atharva\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1716\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Atharva\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1725\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1726\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1730\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[47], line 96\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     93\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m---> 96\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m    100\u001b[0m cls_token_final \u001b[38;5;241m=\u001b[39m x[:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# only cls token\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Atharva\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1716\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Atharva\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1725\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1726\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1730\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[46], line 43\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m---> 43\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Atharva\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1716\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Atharva\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1725\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1726\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1729\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1730\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[44], line 51\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m0\u001b[39m], qkv[\u001b[38;5;241m1\u001b[39m], qkv[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     50\u001b[0m k_t \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m dp \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk_t\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\n\u001b[0;32m     52\u001b[0m attn \u001b[38;5;241m=\u001b[39m dp\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     53\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop(attn)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 810.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 335.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "custom_config = {\n",
    "    \"patch_size\": (1, 10),\n",
    "    \"in_chans\": 1,\n",
    "    \"img_size\": (21,500),\n",
    "    \"n_classes\": 2,\n",
    "    \"embed_dim\": 768,\n",
    "    \"depth\": 12,\n",
    "    \"n_heads\": 12,\n",
    "    \"mlp_ratio\": 4,\n",
    "    \"p\":0.5,\n",
    "    \"attn_p\":0.5\n",
    "}\n",
    "\n",
    "ViT_model = VisionTransformer(**custom_config).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(ViT_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 500\n",
    "batch_size = 16\n",
    "for epoch in range(num_epochs):\n",
    "    ViT_model.train()\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # print(epoch)\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        inputs = X_train[i:i+batch_size].to(device)\n",
    "        labels = y_train[i:i+batch_size].to(device)\n",
    "\n",
    "        # print(epoch , i)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = ViT_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(X_train)\n",
    "    epoch_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {(epoch_accuracy*100):.2f}%\")\n",
    "average_loss = running_loss / len(X_train)\n",
    "print(\"Average Loss:\", average_loss)\n",
    "\n",
    "# Saving model\n",
    "torch.save(ViT_model, 'ViT_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Atharva\\AppData\\Local\\Temp\\ipykernel_24784\\2248634288.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ViT_model = torch.load('ViT_model.pth')\n"
     ]
    }
   ],
   "source": [
    "custom_config = {\n",
    "    \"patch_size\": (1, 100),\n",
    "    \"in_chans\": 1,\n",
    "    \"img_size\": (21,500),\n",
    "    \"n_classes\": 2,\n",
    "    \"embed_dim\": 768,\n",
    "    \"depth\": 12,\n",
    "    \"n_heads\": 12,\n",
    "    \"mlp_ratio\": 4,\n",
    "    \"p\":0.2,\n",
    "    \"attn_p\":0.2\n",
    "}\n",
    "\n",
    "ViT_model = VisionTransformer(**custom_config).to(device)\n",
    "ViT_model = torch.load('ViT_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 50.06%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "ViT_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(X_test)):\n",
    "        inputs = X_test[i:i+1].to(device)\n",
    "        labels = y_test[i:i+1].to(device)\n",
    "        outputs = ViT_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = (correct / total)*100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ViT_model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "classes = ['rest', 'task']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in zip(X_test, y_test):\n",
    "        outputs = ViT_model(inputs.unsqueeze(0))  # Forward pass\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.append(predicted.item())\n",
    "        y_true.append(labels.item())\n",
    "\n",
    "cf_matrix = ViT_model(y_true, y_pred)\n",
    "cf_matrix = cf_matrix.astype('float') / cf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "df_cm = pd.DataFrame(cf_matrix, index=classes, columns=classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sn.heatmap(df_cm, annot=True, cmap='Blues', fmt='.2f')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix_eegnet.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
