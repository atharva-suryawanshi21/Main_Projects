{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './eeg-during-mental-arithmetic-tasks-1.0.0/'\n",
    "\n",
    "rest_filepaths = []\n",
    "task_filepaths = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    if filename.endswith('.edf'):\n",
    "        label = filename.split('_')[-1].split('.')[0]\n",
    "\n",
    "        if label == '1':\n",
    "            rest_filepaths.append(filepath)\n",
    "        else:\n",
    "            task_filepaths.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example function to read and process data\n",
    "def process_data(filepath):\n",
    "    data = mne.io.read_raw_edf(filepath, preload=True)\n",
    "    data.set_eeg_reference()\n",
    "    data.filter(l_freq=0.5, h_freq=45)\n",
    "    \n",
    "    min_t, max_t = 0, 61.99\n",
    "    data.crop(tmin=min_t, tmax=max_t)\n",
    "    \n",
    "    tmin, tmax = 0, 1  # Epoch duration of 1 second\n",
    "    epochs = mne.make_fixed_length_epochs(data, duration=tmax, overlap = 0.5, preload=True)\n",
    "    \n",
    "    return epochs\n",
    "\n",
    "# Function to process all 'task' labeled files\n",
    "def process_task_files(filepaths):\n",
    "    epochs_data = []  # List to store epoch data\n",
    "    labels = []  # List to store corresponding labels\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        epochs = process_data(filepath)\n",
    "        epochs_data.extend(epochs.get_data())\n",
    "        labels.extend([1] * len(epochs))  # Assign label 1 for 'task' (assuming 'task' label)\n",
    "    \n",
    "    return np.array(epochs_data), np.array(labels)\n",
    "\n",
    "def process_rest_files(filepaths):\n",
    "    epochs_data = []  # List to store epoch data\n",
    "    labels = []  # List to store corresponding labels\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        epochs = process_data(filepath)\n",
    "        epochs_data.extend(epochs.get_data())\n",
    "        labels.extend([0] * len(epochs))  # Assign label 1 for 'task' (assuming 'task' label)\n",
    "    \n",
    "    return np.array(epochs_data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "rest_epochs_data, rest_labels = process_rest_files(rest_filepaths)\n",
    "\n",
    "task_epochs_data, task_labels = process_task_files(task_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4392, 21, 500), (4392, 21, 500))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_epochs_data.shape, task_epochs_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4392,), (4392,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_labels.shape , task_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_data_combined = np.concatenate([rest_epochs_data, task_epochs_data], axis=0)\n",
    "\n",
    "# Concatenate labels\n",
    "labels_combined = np.concatenate([rest_labels, task_labels], axis=0)\n",
    "\n",
    "# Shuffle (optional)\n",
    "# Use the same random seed for synchronizing shuffle across data and labels\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "shuffle_indices = np.random.permutation(len(labels_combined))\n",
    "data = epochs_data_combined[shuffle_indices]\n",
    "label = labels_combined[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs shape: (8784, 21, 500)\n",
      "Labels shape: (8784,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Epochs shape:\", data.shape)\n",
    "print(\"Labels shape:\", label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.load('data.npy')\n",
    "# label = np.load('label.npy')\n",
    "\n",
    "# # # Convert numpy arrays to PyTorch tensors\n",
    "# # data = torch.tensor(data, dtype=torch.float32)\n",
    "# # label = torch.tensor(label, dtype=torch.long)  # Assuming labels are integers (dtype=torch.long)\n",
    "\n",
    "# # Print shapes to verify\n",
    "# print(\"Epochs shape:\", data.shape)\n",
    "# print(\"Labels shape:\", label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_train: torch.Size([7027, 1, 21, 500])\n",
      "Size of X_test: torch.Size([1757, 1, 21, 500])\n",
      "Size of y_train: torch.Size([7027])\n",
      "Size of y_test: torch.Size([1757])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import  TensorDataset\n",
    "\n",
    "# Choosing Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# # Normalizing Labels to [0, 1, 2, 3]\n",
    "# y = labels - np.min(labels)\n",
    "y = label\n",
    "\n",
    "# Normalizing Input features: z-score(mean=0, std=1)\n",
    "X = (data - np.mean(data)) / np.std(data)\n",
    "\n",
    "# Checking the existance of null & inf in the dataset\n",
    "if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n",
    "    raise ValueError(\"Data contains NaNs or infinities after normalization.\")\n",
    "if np.any(np.isnan(y)) or np.any(np.isinf(y)):\n",
    "    raise ValueError(\"Labels contain NaNs or infinities.\")\n",
    "\n",
    "# Making the X,y tensors for K-Fold Cross Validation\n",
    "X_tensor = torch.Tensor(X).unsqueeze(1)\n",
    "y_tensor = torch.LongTensor(y)\n",
    "\n",
    "# Spliting  Data: 80% for Train and 20% for Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Converting to Tensor\n",
    "X_train = torch.Tensor(X_train).unsqueeze(1).to(device)\n",
    "X_test = torch.Tensor(X_test).unsqueeze(1).to(device)\n",
    "y_train = torch.LongTensor(y_train).to(device)\n",
    "y_test = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "# Creating Tensor Dataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Printing the sizes\n",
    "print(\"Size of X_train:\", X_train.size())\n",
    "print(\"Size of X_test:\", X_test.size())\n",
    "print(\"Size of y_train:\", y_train.size())\n",
    "print(\"Size of y_test:\", y_test.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchEmbed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Split image into patches and then embed them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Size of the image (it is a square).\n",
    "\n",
    "    patch_size : int\n",
    "        Size of the patch (it is a square).\n",
    "\n",
    "    in_chans : int\n",
    "        Number of input channels.\n",
    "\n",
    "    embed_dim : int\n",
    "        The emmbedding dimension.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_patches : int\n",
    "        Number of patches inside of our image.\n",
    "\n",
    "    proj : nn.Conv2d\n",
    "        Convolutional layer that does both the splitting into patches\n",
    "        and their embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        # self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "                in_chans,\n",
    "                embed_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, in_chans, img_size, img_size)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches, embed_dim)`.\n",
    "        \"\"\"\n",
    "        x = self.proj(\n",
    "                x\n",
    "            )  # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        x = x.flatten(2)  # (n_samples, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (n_samples, n_patches, embed_dim)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        The input and out dimension of per token features.\n",
    "\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "\n",
    "    attn_p : float\n",
    "        Dropout probability applied to the query, key and value tensors.\n",
    "\n",
    "    proj_p : float\n",
    "        Dropout probability applied to the output tensor.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    scale : float\n",
    "        Normalizing consant for the dot product.\n",
    "\n",
    "    qkv : nn.Linear\n",
    "        Linear projection for the query, key and value.\n",
    "\n",
    "    proj : nn.Linear\n",
    "        Linear mapping that takes in the concatenated output of all attention\n",
    "        heads and maps it into a new space.\n",
    "\n",
    "    attn_drop, proj_drop : nn.Dropout\n",
    "        Dropout layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "        \"\"\"\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "\n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "\n",
    "        qkv = self.qkv(x)  # (n_samples, n_patches + 1, 3 * dim)\n",
    "        qkv = qkv.reshape(\n",
    "                n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
    "        )  # (n_smaples, n_patches + 1, 3, n_heads, head_dim)\n",
    "        qkv = qkv.permute(\n",
    "                2, 0, 3, 1, 4\n",
    "        )  # (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        k_t = k.transpose(-2, -1)  # (n_samples, n_heads, head_dim, n_patches + 1)\n",
    "        dp = (\n",
    "           q @ k_t\n",
    "        ) * self.scale # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "        attn = dp.softmax(dim=-1)  # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        weighted_avg = attn @ v  # (n_samples, n_heads, n_patches +1, head_dim)\n",
    "        weighted_avg = weighted_avg.transpose(\n",
    "                1, 2\n",
    "        )  # (n_samples, n_patches + 1, n_heads, head_dim)\n",
    "        weighted_avg = weighted_avg.flatten(2)  # (n_samples, n_patches + 1, dim)\n",
    "\n",
    "        x = self.proj(weighted_avg)  # (n_samples, n_patches + 1, dim)\n",
    "        x = self.proj_drop(x)  # (n_samples, n_patches + 1, dim)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multilayer perceptron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Number of input features.\n",
    "\n",
    "    hidden_features : int\n",
    "        Number of nodes in the hidden layer.\n",
    "\n",
    "    out_features : int\n",
    "        Number of output features.\n",
    "\n",
    "    p : float\n",
    "        Dropout probability.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    fc : nn.Linear\n",
    "        The First linear layer.\n",
    "\n",
    "    act : nn.GELU\n",
    "        GELU activation function.\n",
    "\n",
    "    fc2 : nn.Linear\n",
    "        The second linear layer.\n",
    "\n",
    "    drop : nn.Dropout\n",
    "        Dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, in_features)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches +1, out_features)`\n",
    "        \"\"\"\n",
    "        x = self.fc1(\n",
    "                x\n",
    "        ) # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.act(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.fc2(x)  # (n_samples, n_patches + 1, out_features)\n",
    "        x = self.drop(x)  # (n_samples, n_patches + 1, out_features)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Embeddinig dimension.\n",
    "\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension size of the `MLP` module with respect\n",
    "        to `dim`.\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "\n",
    "    p, attn_p : float\n",
    "        Dropout probability.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    norm1, norm2 : LayerNorm\n",
    "        Layer normalization.\n",
    "\n",
    "    attn : Attention\n",
    "        Attention module.\n",
    "\n",
    "    mlp : MLP\n",
    "        MLP module.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(\n",
    "                dim,\n",
    "                n_heads=n_heads,\n",
    "                qkv_bias=qkv_bias,\n",
    "                attn_p=attn_p,\n",
    "                proj_p=p\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "                in_features=dim,\n",
    "                hidden_features=hidden_features,\n",
    "                out_features=dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "        \"\"\"\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Simplified implementation of the Vision transformer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Both height and the width of the image (it is a square).\n",
    "\n",
    "    patch_size : int\n",
    "        Both height and the width of the patch (it is a square).\n",
    "\n",
    "    in_chans : int\n",
    "        Number of input channels.\n",
    "\n",
    "    n_classes : int\n",
    "        Number of classes.\n",
    "\n",
    "    embed_dim : int\n",
    "        Dimensionality of the token/patch embeddings.\n",
    "\n",
    "    depth : int\n",
    "        Number of blocks.\n",
    "\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension of the `MLP` module.\n",
    "\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "\n",
    "    p, attn_p : float\n",
    "        Dropout probability.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patch_embed : PatchEmbed\n",
    "        Instance of `PatchEmbed` layer.\n",
    "\n",
    "    cls_token : nn.Parameter\n",
    "        Learnable parameter that will represent the first token in the sequence.\n",
    "        It has `embed_dim` elements.\n",
    "\n",
    "    pos_emb : nn.Parameter\n",
    "        Positional embedding of the cls token + all the patches.\n",
    "        It has `(n_patches + 1) * embed_dim` elements.\n",
    "\n",
    "    pos_drop : nn.Dropout\n",
    "        Dropout layer.\n",
    "\n",
    "    blocks : nn.ModuleList\n",
    "        List of `Block` modules.\n",
    "\n",
    "    norm : nn.LayerNorm\n",
    "        Layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=384,\n",
    "            patch_size=16,\n",
    "            in_chans=3,\n",
    "            n_classes=1000,\n",
    "            embed_dim=768,\n",
    "            depth=12,\n",
    "            n_heads=12,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=True,\n",
    "            p=0.,\n",
    "            attn_p=0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size,\n",
    "                patch_size=patch_size,\n",
    "                in_chans=in_chans,\n",
    "                embed_dim=embed_dim,\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dim,\n",
    "                    n_heads=n_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    p=p,\n",
    "                    attn_p=attn_p,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run the forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, in_chans, img_size, img_size)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor\n",
    "            Logits over all the classes - `(n_samples, n_classes)`.\n",
    "        \"\"\"\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_token = self.cls_token.expand(\n",
    "                n_samples, -1, -1\n",
    "        )  # (n_samples, 1, embed_dim)\n",
    "        x = torch.cat((cls_token, x), dim=1)  # (n_samples, 1 + n_patches, embed_dim)\n",
    "        x = x + self.pos_embed  # (n_samples, 1 + n_patches, embed_dim)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        cls_token_final = x[:, 0]  # just the CLS token\n",
    "        x = self.head(cls_token_final)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "custom_config = {\n",
    "            \"img_size\":(21,500),\n",
    "            \"patch_size\":(1,25),\n",
    "            \"in_chans\":1,\n",
    "            \"n_classes\":2,\n",
    "            \"embed_dim\":768,\n",
    "            \"depth\":12,\n",
    "            \"n_heads\":12,\n",
    "            \"mlp_ratio\":4.,\n",
    "            \"qkv_bias\":True,\n",
    "            \"p\":0.,\n",
    "            \"attn_p\":0.,\n",
    "}\n",
    "ViT_model = VisionTransformer(**custom_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "random_input = torch.randn(8, 1, 21, 500).to(device)\n",
    "\n",
    "# Pass the random input through the model to get the output\n",
    "output = ViT_model(random_input)\n",
    "\n",
    "# Print the output\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(ViT_model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 10\n",
    "# best_test_acc = 0.0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     ViT_model.train()\n",
    "#     train_loss = 0.0\n",
    "#     correct_train = 0\n",
    "\n",
    "#     for inputs, labels in tqdm(train_loader):\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = ViT_model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loss += loss.item() * inputs.size(0)\n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "#         correct_train += (preds == labels).sum().item()\n",
    "\n",
    "\n",
    "#     train_loss = train_loss / len(train_loader.dataset)\n",
    "#     train_acc = correct_train / len(train_loader.dataset)\n",
    "\n",
    "#     ViT_model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     correct_test = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = ViT_model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             correct_test += (preds == labels).sum().item()\n",
    "            \n",
    "            \n",
    "\n",
    "#     test_loss = test_loss / len(test_loader.dataset)\n",
    "#     test_acc = correct_test / len(test_loader.dataset)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#     print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "#     print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#     # Save checkpoint\n",
    "#     checkpoint = {\n",
    "#         'epoch': epoch + 1,\n",
    "#         'model_state_dict': ViT_model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'train_loss': train_loss,\n",
    "#         'train_acc': train_acc,\n",
    "#         'test_loss': test_loss,\n",
    "#         'test_acc': test_acc\n",
    "#     }\n",
    "#     torch.save(checkpoint, f'./results/checkpoint_epoch_{epoch + 1}.pth')\n",
    "\n",
    "#     # Save best model based on test accuracy\n",
    "#     if test_acc > best_test_acc:\n",
    "#         best_test_acc = test_acc\n",
    "#         torch.save(checkpoint, './results/best_model.pth')\n",
    "\n",
    "# print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Atharva\\AppData\\Local\\Temp\\ipykernel_42364\\4071669316.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('./results/best_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch 4\n",
      "Train Loss: 0.6151, Train Accuracy: 0.6556\n",
      "Test Loss: 0.5592, Test Accuracy: 0.7131\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ViT_model.parameters(), lr=1e-4)\n",
    "num_epochs = 20\n",
    "\n",
    "checkpoint = torch.load('./results/best_model.pth')\n",
    "ViT_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch']\n",
    "train_loss = checkpoint['train_loss']\n",
    "train_acc = checkpoint['train_acc']\n",
    "test_loss = checkpoint['test_loss']\n",
    "test_acc = checkpoint['test_acc']\n",
    "\n",
    "print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# # If you want to resume training\n",
    "# ViT_model.train()\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     ViT_model.train()\n",
    "#     train_loss = 0.0\n",
    "#     correct_train = 0\n",
    "\n",
    "#     for inputs, labels in tqdm(train_loader):\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = ViT_model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         train_loss += loss.item() * inputs.size(0)\n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "#         correct_train += (preds == labels).sum().item()\n",
    "\n",
    "\n",
    "#     train_loss = train_loss / len(train_loader.dataset)\n",
    "#     train_acc = correct_train / len(train_loader.dataset)\n",
    "\n",
    "#     ViT_model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     correct_test = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = ViT_model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             correct_test += (preds == labels).sum().item()\n",
    "            \n",
    "            \n",
    "\n",
    "#     test_loss = test_loss / len(test_loader.dataset)\n",
    "#     test_acc = correct_test / len(test_loader.dataset)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#     print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "#     print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#     # Save checkpoint\n",
    "#     checkpoint = {\n",
    "#         'epoch': epoch + 1,\n",
    "#         'model_state_dict': ViT_model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'train_loss': train_loss,\n",
    "#         'train_acc': train_acc,\n",
    "#         'test_loss': test_loss,\n",
    "#         'test_acc': test_acc\n",
    "#     }\n",
    "#     torch.save(checkpoint, f'./results/checkpoint_epoch_{epoch + 1}.pth')\n",
    "\n",
    "#     # Save best model based on test accuracy\n",
    "#     if test_acc > best_test_acc:\n",
    "#         best_test_acc = test_acc\n",
    "#         torch.save(checkpoint, './results/best_model.pth')\n",
    "\n",
    "# print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Loss: 0.5592, Final Test Accuracy: 0.7131\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "# Evaluate the model on test data and generate confusion matrix\n",
    "ViT_model.eval()\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = ViT_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_test += (preds == labels).sum().item()\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "test_acc = correct_test / len(test_loader.dataset)\n",
    "\n",
    "print(f\"Final Test Loss: {test_loss:.4f}, Final Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
