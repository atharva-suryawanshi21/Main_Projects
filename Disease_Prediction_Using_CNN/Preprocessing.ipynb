{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "1. Read images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3302 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"potato_dataset\",\n",
    "    labels='inferred',\n",
    "    label_mode = 'int',\n",
    "    batch_size = batch_size,\n",
    "    image_size = (image_size,image_size),\n",
    "    shuffle = True,\n",
    "    seed= 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['early_blight', 'late_blight', 'normal']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = dataset.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the data is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 1149\n",
      "Class 0: 1133\n",
      "Class 2: 1020\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store class counts\n",
    "class_counts = {}\n",
    "\n",
    "# Loop through the dataset and count elements in each class\n",
    "for images, labels in dataset:\n",
    "    for label in labels.numpy():\n",
    "        if label not in class_counts:\n",
    "            class_counts[label] = 0\n",
    "        class_counts[label] += 1\n",
    "\n",
    "# Print the counts for each class\n",
    "for label, count in class_counts.items():\n",
    "    print(f'Class {label}: {count}')\n",
    "\n",
    "min_count = min(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is distributed in acceptable range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, train_split = 0.8, val_split = 0.1, shuffle = True, shuffle_size= 10000):\n",
    "    train_count = int(len(dataset)*train_split)\n",
    "    val_count = int(len(dataset)*val_split)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_size, seed = 42)\n",
    "\n",
    "    train_dataset = dataset.take(train_count)\n",
    "    val_test_dataset = dataset.skip(train_count)\n",
    "\n",
    "    validation_dataset = val_test_dataset.take(val_count)\n",
    "    test_dataset = val_test_dataset.skip(val_count)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size = 83\n",
      "validation size = 10\n",
      "test size = 11\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = split_data(dataset)\n",
    "print(f\"train size = {len(train)}\")\n",
    "print(f\"validation size = {len(validation)}\")\n",
    "print(f\"test size = {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache and prefetch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.cache().shuffle(10000).prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "validation = validation.cache().shuffle(10000).prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "test = test.cache().shuffle(10000).prefetch(buffer_size = tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce layers for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_rescale_layer = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.Resizing(image_size,image_size),\n",
    "    layers.experimental.preprocessing.Rescaling(1.0/255)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_layer = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.3),\n",
    "    layers.RandomZoom(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "input_shape = (batch_size, image_size,image_size, 3)\n",
    "\n",
    "def conv2D_layer(filter):\n",
    "    return tf.keras.layers.Conv2D(\n",
    "        filters = filter, \n",
    "        kernel_size=(3,3),\n",
    "        padding=\"valid\", # no padding\n",
    "        activation=\"relu\",\n",
    "        input_shape = input_shape\n",
    "    )\n",
    "    \n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # preprocessing layers\n",
    "    resize_rescale_layer,\n",
    "    data_augmentation_layer,\n",
    "\n",
    "    # Convolutional layer\n",
    "    conv2D_layer(128),\n",
    "    tf.keras.layers.MaxPool2D((2,2)),\n",
    "    conv2D_layer(64),\n",
    "    tf.keras.layers.MaxPool2D((2,2)),\n",
    "    conv2D_layer(64),\n",
    "    tf.keras.layers.MaxPool2D((2,2)),\n",
    "    conv2D_layer(64),\n",
    "    tf.keras.layers.MaxPool2D((2,2)),\n",
    "    conv2D_layer(32),\n",
    "    tf.keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # Dense layer\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.build(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_8 (Sequential)   (32, 256, 256, 3)         0         \n",
      "                                                                 \n",
      " sequential_9 (Sequential)   (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 254, 254, 128)     3584      \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 127, 127, 128)    0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 125, 125, 64)      73792     \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 62, 62, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, 30, 30, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPoolin  (None, 14, 14, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 12, 12, 32)        18464     \n",
      "                                                                 \n",
      " max_pooling2d_24 (MaxPoolin  (None, 6, 6, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                36896     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206,691\n",
      "Trainable params: 206,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "83/83 [==============================] - 304s 4s/step - loss: 1.0701 - accuracy: 0.3947 - val_loss: 1.0546 - val_accuracy: 0.4313\n",
      "Epoch 2/10\n",
      "83/83 [==============================] - 315s 4s/step - loss: 1.0036 - accuracy: 0.5156 - val_loss: 0.9606 - val_accuracy: 0.5625\n",
      "Epoch 3/10\n",
      "83/83 [==============================] - 309s 4s/step - loss: 0.9216 - accuracy: 0.5779 - val_loss: 0.7970 - val_accuracy: 0.6969\n",
      "Epoch 4/10\n",
      "83/83 [==============================] - 317s 4s/step - loss: 0.7483 - accuracy: 0.7034 - val_loss: 0.5334 - val_accuracy: 0.7656\n",
      "Epoch 5/10\n",
      "83/83 [==============================] - 288s 4s/step - loss: 0.5670 - accuracy: 0.7730 - val_loss: 0.5631 - val_accuracy: 0.7656\n",
      "Epoch 6/10\n",
      "83/83 [==============================] - 322s 4s/step - loss: 0.4027 - accuracy: 0.8399 - val_loss: 0.4634 - val_accuracy: 0.8219\n",
      "Epoch 7/10\n",
      "83/83 [==============================] - 321s 4s/step - loss: 0.3789 - accuracy: 0.8532 - val_loss: 0.2708 - val_accuracy: 0.8750\n",
      "Epoch 8/10\n",
      "83/83 [==============================] - 287s 3s/step - loss: 0.2915 - accuracy: 0.8852 - val_loss: 0.1864 - val_accuracy: 0.9344\n",
      "Epoch 9/10\n",
      "83/83 [==============================] - 298s 4s/step - loss: 0.2564 - accuracy: 0.9137 - val_loss: 0.1348 - val_accuracy: 0.9563\n",
      "Epoch 10/10\n",
      "83/83 [==============================] - 288s 3s/step - loss: 0.2069 - accuracy: 0.9281 - val_loss: 0.1723 - val_accuracy: 0.9563\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "EPOCHS =10\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs= EPOCHS,\n",
    "    batch_size=batch_size,\n",
    "    verbose =1,\n",
    "    validation_data=validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 8s 630ms/step - loss: 0.1823 - accuracy: 0.9602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18231211602687836, 0.9602272510528564]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "loss= history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\GITHUB\\Main_Projects\\Disease_Prediction_Using_CNN\\Preprocessing.ipynb Cell 26\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GITHUB/Main_Projects/Disease_Prediction_Using_CNN/Preprocessing.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m,\u001b[39m8\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GITHUB/Main_Projects/Disease_Prediction_Using_CNN/Preprocessing.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/GITHUB/Main_Projects/Disease_Prediction_Using_CNN/Preprocessing.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(EPOCHS), accuracy, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining Accuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GITHUB/Main_Projects/Disease_Prediction_Using_CNN/Preprocessing.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(EPOCHS), val_accuracy, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidation Accuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GITHUB/Main_Projects/Disease_Prediction_Using_CNN/Preprocessing.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mlegend(loc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlower right\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAKZCAYAAAD9MDPMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAd0UlEQVR4nO3df2yW5b348U+h0KrntEaYFQQ79OjGRuZGCYxyyDKPdkHjQrITu7iIOk3W7AdCp2cwTnQQk2Y7mTlzE/ZD0CxB1/gz/tE5+8emKG7nwMqyDBIXYRa2ImmNLepOEbjPH35pvl2L61P6oYXzeiXPH8/ldT/Pddntnbt379yWFUVRBABjbtJ4LwDgbCWwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASUoO7AsvvBDXX399zJw5M8rKyuLpp5/+u8c8//zzUVdXF5WVlXHppZfGD3/4w9GsFeCMUnJg33777bjyyivjBz/4wYjm79u3L6699tpYunRpdHR0xDe/+c1YuXJlPPHEEyUvFuBMUnYqD3spKyuLp556KpYvX37SOd/4xjfimWeeiT179gyMNTU1xe9+97t4+eWXR/vVABNeefYXvPzyy9HQ0DBo7DOf+Uxs3rw53n333ZgyZcqQY/r7+6O/v3/g/fHjx+ONN96IadOmRVlZWfaSgf+DiqKIw4cPx8yZM2PSpLH581R6YA8ePBg1NTWDxmpqauLo0aPR3d0dM2bMGHJMS0tLrF+/PntpAEPs378/Zs2aNSaflR7YiBhy1nniqsTJzkbXrl0bzc3NA+97e3vjkksuif3790dVVVXeQoH/s/r6+mL27Nnxj//4j2P2memBveiii+LgwYODxg4dOhTl5eUxbdq0YY+pqKiIioqKIeNVVVUCC6Qay8uQ6ffBLl68ONrb2weNPffcc7FgwYJhr78CnC1KDuxbb70Vu3btil27dkXEe7dh7dq1Kzo7OyPivV/vV6xYMTC/qakpXnvttWhubo49e/bEli1bYvPmzXHnnXeOzQ4AJqiSLxHs2LEjPv3pTw+8P3Gt9Oabb46HH344urq6BmIbETFnzpxoa2uL1atXxwMPPBAzZ86M+++/Pz73uc+NwfIBJq5Tug/2dOnr64vq6uro7e11DRZIkdEZzyIASCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQJJRBXbjxo0xZ86cqKysjLq6uti2bdv7zt+6dWtceeWVce6558aMGTPi1ltvjZ6enlEtGOBMUXJgW1tbY9WqVbFu3bro6OiIpUuXxrJly6Kzs3PY+S+++GKsWLEibrvttvjDH/4Qjz32WPz3f/933H777ae8eICJrOTA3nfffXHbbbfF7bffHnPnzo3//M//jNmzZ8emTZuGnf/rX/86PvjBD8bKlStjzpw58c///M/xpS99KXbs2HHKiweYyEoK7JEjR2Lnzp3R0NAwaLyhoSG2b98+7DH19fVx4MCBaGtri6Io4vXXX4/HH388rrvuupN+T39/f/T19Q16AZxpSgpsd3d3HDt2LGpqagaN19TUxMGDB4c9pr6+PrZu3RqNjY0xderUuOiii+L888+P73//+yf9npaWlqiurh54zZ49u5RlAkwIo/ojV1lZ2aD3RVEMGTth9+7dsXLlyrj77rtj586d8eyzz8a+ffuiqanppJ+/du3a6O3tHXjt379/NMsEGFflpUyePn16TJ48ecjZ6qFDh4ac1Z7Q0tISS5YsibvuuisiIj72sY/FeeedF0uXLo177703ZsyYMeSYioqKqKioKGVpABNOSWewU6dOjbq6umhvbx803t7eHvX19cMe884778SkSYO/ZvLkyRHx3pkvwNmq5EsEzc3N8eCDD8aWLVtiz549sXr16ujs7Bz4lX/t2rWxYsWKgfnXX399PPnkk7Fp06bYu3dvvPTSS7Fy5cpYuHBhzJw5c+x2AjDBlHSJICKisbExenp6YsOGDdHV1RXz5s2Ltra2qK2tjYiIrq6uQffE3nLLLXH48OH4wQ9+EF//+tfj/PPPj6uuuiq+/e1vj90uACagsuIM+D29r68vqquro7e3N6qqqsZ7OcBZKKMznkUAkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCSjCuzGjRtjzpw5UVlZGXV1dbFt27b3nd/f3x/r1q2L2traqKioiMsuuyy2bNkyqgUDnCnKSz2gtbU1Vq1aFRs3bowlS5bEj370o1i2bFns3r07LrnkkmGPueGGG+L111+PzZs3xz/90z/FoUOH4ujRo6e8eICJrKwoiqKUAxYtWhTz58+PTZs2DYzNnTs3li9fHi0tLUPmP/vss/H5z38+9u7dGxdccMGoFtnX1xfV1dXR29sbVVVVo/oMgPeT0ZmSLhEcOXIkdu7cGQ0NDYPGGxoaYvv27cMe88wzz8SCBQviO9/5Tlx88cVxxRVXxJ133hl//etfR79qgDNASZcIuru749ixY1FTUzNovKamJg4ePDjsMXv37o0XX3wxKisr46mnnoru7u748pe/HG+88cZJr8P29/dHf3//wPu+vr5SlgkwIYzqj1xlZWWD3hdFMWTshOPHj0dZWVls3bo1Fi5cGNdee23cd9998fDDD5/0LLalpSWqq6sHXrNnzx7NMgHGVUmBnT59ekyePHnI2eqhQ4eGnNWeMGPGjLj44oujurp6YGzu3LlRFEUcOHBg2GPWrl0bvb29A6/9+/eXskyACaGkwE6dOjXq6uqivb190Hh7e3vU19cPe8ySJUviL3/5S7z11lsDY6+88kpMmjQpZs2aNewxFRUVUVVVNegFcKYp+RJBc3NzPPjgg7Fly5bYs2dPrF69Ojo7O6OpqSki3jv7XLFixcD8G2+8MaZNmxa33npr7N69O1544YW466674otf/GKcc845Y7cTgAmm5PtgGxsbo6enJzZs2BBdXV0xb968aGtri9ra2oiI6Orqis7OzoH5//AP/xDt7e3xta99LRYsWBDTpk2LG264Ie69996x2wXABFTyfbDjwX2wQLZxvw8WgJETWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkGVVgN27cGHPmzInKysqoq6uLbdu2jei4l156KcrLy+PjH//4aL4W4IxScmBbW1tj1apVsW7duujo6IilS5fGsmXLorOz832P6+3tjRUrVsS//Mu/jHqxAGeSsqIoilIOWLRoUcyfPz82bdo0MDZ37txYvnx5tLS0nPS4z3/+83H55ZfH5MmT4+mnn45du3aN+Dv7+vqiuro6ent7o6qqqpTlAoxIRmdKOoM9cuRI7Ny5MxoaGgaNNzQ0xPbt20963EMPPRSvvvpq3HPPPSP6nv7+/ujr6xv0AjjTlBTY7u7uOHbsWNTU1Awar6mpiYMHDw57zB//+MdYs2ZNbN26NcrLy0f0PS0tLVFdXT3wmj17dinLBJgQRvVHrrKyskHvi6IYMhYRcezYsbjxxhtj/fr1ccUVV4z489euXRu9vb0Dr/37949mmQDjamSnlP/P9OnTY/LkyUPOVg8dOjTkrDYi4vDhw7Fjx47o6OiIr371qxERcfz48SiKIsrLy+O5556Lq666ashxFRUVUVFRUcrSACacks5gp06dGnV1ddHe3j5ovL29Perr64fMr6qqit///vexa9eugVdTU1N86EMfil27dsWiRYtObfUAE1hJZ7AREc3NzXHTTTfFggULYvHixfHjH/84Ojs7o6mpKSLe+/X+z3/+c/z0pz+NSZMmxbx58wYdf+GFF0ZlZeWQcYCzTcmBbWxsjJ6entiwYUN0dXXFvHnzoq2tLWprayMioqur6+/eEwvwf0HJ98GOB/fBAtnG/T5YAEZOYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkowrsxo0bY86cOVFZWRl1dXWxbdu2k8598skn45prrokPfOADUVVVFYsXL45f/OIXo14wwJmi5MC2trbGqlWrYt26ddHR0RFLly6NZcuWRWdn57DzX3jhhbjmmmuira0tdu7cGZ/+9Kfj+uuvj46OjlNePMBEVlYURVHKAYsWLYr58+fHpk2bBsbmzp0by5cvj5aWlhF9xkc/+tFobGyMu+++e0Tz+/r6orq6Onp7e6OqqqqU5QKMSEZnSjqDPXLkSOzcuTMaGhoGjTc0NMT27dtH9BnHjx+Pw4cPxwUXXHDSOf39/dHX1zfoBXCmKSmw3d3dcezYsaipqRk0XlNTEwcPHhzRZ3z3u9+Nt99+O2644YaTzmlpaYnq6uqB1+zZs0tZJsCEMKo/cpWVlQ16XxTFkLHhPProo/Gtb30rWltb48ILLzzpvLVr10Zvb+/Aa//+/aNZJsC4Ki9l8vTp02Py5MlDzlYPHTo05Kz2b7W2tsZtt90Wjz32WFx99dXvO7eioiIqKipKWRrAhFPSGezUqVOjrq4u2tvbB423t7dHfX39SY979NFH45ZbbolHHnkkrrvuutGtFOAMU9IZbEREc3Nz3HTTTbFgwYJYvHhx/PjHP47Ozs5oamqKiPd+vf/zn/8cP/3pTyPivbiuWLEivve978UnP/nJgbPfc845J6qrq8dwKwATS8mBbWxsjJ6entiwYUN0dXXFvHnzoq2tLWprayMioqura9A9sT/60Y/i6NGj8ZWvfCW+8pWvDIzffPPN8fDDD5/6DgAmqJLvgx0P7oMFso37fbAAjJzAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiCJwAIkEViAJAILkERgAZIILEASgQVIIrAASQQWIInAAiQRWIAkAguQRGABkggsQBKBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAElGFdiNGzfGnDlzorKyMurq6mLbtm3vO//555+Purq6qKysjEsvvTR++MMfjmqxAGeSkgPb2toaq1atinXr1kVHR0csXbo0li1bFp2dncPO37dvX1x77bWxdOnS6OjoiG9+85uxcuXKeOKJJ0558QATWVlRFEUpByxatCjmz58fmzZtGhibO3duLF++PFpaWobM/8Y3vhHPPPNM7NmzZ2Csqakpfve738XLL788ou/s6+uL6urq6O3tjaqqqlKWCzAiGZ0pL2XykSNHYufOnbFmzZpB4w0NDbF9+/Zhj3n55ZejoaFh0NhnPvOZ2Lx5c7z77rsxZcqUIcf09/dHf3//wPve3t6IeO9fAECGE30p8ZzzfZUU2O7u7jh27FjU1NQMGq+pqYmDBw8Oe8zBgweHnX/06NHo7u6OGTNmDDmmpaUl1q9fP2R89uzZpSwXoGQ9PT1RXV09Jp9VUmBPKCsrG/S+KIohY39v/nDjJ6xduzaam5sH3r/55ptRW1sbnZ2dY7bxiaSvry9mz54d+/fvP2svgZzte7S/M19vb29ccsklccEFF4zZZ5YU2OnTp8fkyZOHnK0eOnRoyFnqCRdddNGw88vLy2PatGnDHlNRUREVFRVDxqurq8/aH25ERFVV1Vm9v4izf4/2d+abNGns7l4t6ZOmTp0adXV10d7ePmi8vb096uvrhz1m8eLFQ+Y/99xzsWDBgmGvvwKcLUpOdXNzczz44IOxZcuW2LNnT6xevTo6OzujqakpIt779X7FihUD85uamuK1116L5ubm2LNnT2zZsiU2b94cd95559jtAmACKvkabGNjY/T09MSGDRuiq6sr5s2bF21tbVFbWxsREV1dXYPuiZ0zZ060tbXF6tWr44EHHoiZM2fG/fffH5/73OdG/J0VFRVxzz33DHvZ4Gxwtu8v4uzfo/2d+TL2WPJ9sACMjGcRACQRWIAkAguQRGABkkyYwJ7tj0AsZX9PPvlkXHPNNfGBD3wgqqqqYvHixfGLX/ziNK62dKX+/E546aWXory8PD7+8Y/nLnAMlLrH/v7+WLduXdTW1kZFRUVcdtllsWXLltO02tKVur+tW7fGlVdeGeeee27MmDEjbr311ujp6TlNqy3NCy+8ENdff33MnDkzysrK4umnn/67x4xJY4oJ4Gc/+1kxZcqU4ic/+Umxe/fu4o477ijOO++84rXXXht2/t69e4tzzz23uOOOO4rdu3cXP/nJT4opU6YUjz/++Gle+ciUur877rij+Pa3v13813/9V/HKK68Ua9euLaZMmVL89re/Pc0rH5lS93fCm2++WVx66aVFQ0NDceWVV56exY7SaPb42c9+tli0aFHR3t5e7Nu3r/jNb35TvPTSS6dx1SNX6v62bdtWTJo0qfje975X7N27t9i2bVvx0Y9+tFi+fPlpXvnItLW1FevWrSueeOKJIiKKp5566n3nj1VjJkRgFy5cWDQ1NQ0a+/CHP1ysWbNm2Pn/9m//Vnz4wx8eNPalL32p+OQnP5m2xlNR6v6G85GPfKRYv379WC9tTIx2f42NjcW///u/F/fcc8+ED2ype/z5z39eVFdXFz09Padjeaes1P39x3/8R3HppZcOGrv//vuLWbNmpa1xrIwksGPVmHG/RHDiEYh/+0jD0TwCcceOHfHuu++mrXU0RrO/v3X8+PE4fPjwmD6EYqyMdn8PPfRQvPrqq3HPPfdkL/GUjWaPzzzzTCxYsCC+853vxMUXXxxXXHFF3HnnnfHXv/71dCy5JKPZX319fRw4cCDa2tqiKIp4/fXX4/HHH4/rrrvudCw53Vg1ZlRP0xpLp+sRiONlNPv7W9/97nfj7bffjhtuuCFjiadkNPv74x//GGvWrIlt27ZFefm4/0/w7xrNHvfu3RsvvvhiVFZWxlNPPRXd3d3x5S9/Od54440Jdx12NPurr6+PrVu3RmNjY/zP//xPHD16ND772c/G97///dOx5HRj1ZhxP4M9IfsRiOOt1P2d8Oijj8a3vvWtaG1tjQsvvDBreadspPs7duxY3HjjjbF+/fq44oorTtfyxkQpP8Pjx49HWVlZbN26NRYuXBjXXntt3HffffHwww9PyLPYiNL2t3v37li5cmXcfffdsXPnznj22Wdj3759A88kORuMRWPG/fThdD0CcbyMZn8ntLa2xm233RaPPfZYXH311ZnLHLVS93f48OHYsWNHdHR0xFe/+tWIeC9GRVFEeXl5PPfcc3HVVVedlrWP1Gh+hjNmzIiLL7540POL586dG0VRxIEDB+Lyyy9PXXMpRrO/lpaWWLJkSdx1110REfGxj30szjvvvFi6dGnce++9E+q3yNEYq8aM+xns2f4IxNHsL+K9M9dbbrklHnnkkQl9XavU/VVVVcXvf//72LVr18CrqakpPvShD8WuXbti0aJFp2vpIzaan+GSJUviL3/5S7z11lsDY6+88kpMmjQpZs2albreUo1mf++8886Q56ZOnjw5Isb2P7kyXsasMSX9SSzJiVtENm/eXOzevbtYtWpVcd555xV/+tOfiqIoijVr1hQ33XTTwPwTt1CsXr262L17d7F58+Yz4jatke7vkUceKcrLy4sHHnig6OrqGni9+eab47WF91Xq/v7WmXAXQal7PHz4cDFr1qziX//1X4s//OEPxfPPP19cfvnlxe233z5eW3hfpe7voYceKsrLy4uNGzcWr776avHiiy8WCxYsKBYuXDheW3hfhw8fLjo6OoqOjo4iIor77ruv6OjoGLgNLasxEyKwRVEUDzzwQFFbW1tMnTq1mD9/fvH8888P/LObb765+NSnPjVo/q9+9aviE5/4RDF16tTigx/8YLFp06bTvOLSlLK/T33qU0VEDHndfPPNp3/hI1Tqz+//dyYEtihK3+OePXuKq6++ujjnnHOKWbNmFc3NzcU777xzmlc9cqXu7/777y8+8pGPFOecc04xY8aM4gtf+EJx4MCB07zqkfnlL3/5vv+fymqMxxUCJBn3a7AAZyuBBUgisABJBBYgicACJBFYgCQCC5BEYAGSCCxAEoEFSCKwAEkEFiDJ/wJ3uLdDoXu+XwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(EPOCHS), accuracy, label='Training Accuracy')\n",
    "plt.plot(range(EPOCHS), val_accuracy, label=\"Validation Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(EPOCHS), loss, label='Training Loss')\n",
    "plt.plot(range(EPOCHS), val_loss, label=\"Validation Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title('Training and Validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prediction(model, img):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = round(100*(np.argmax(predictions[0])), 2)\n",
    "\n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = 1\n",
    "model.save(f\"../saved_models/{model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
