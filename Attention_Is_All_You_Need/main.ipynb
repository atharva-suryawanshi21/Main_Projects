{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ironman/anaconda3/envs/verti_att/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir = 'logs')\n",
    "import re\n",
    "\n",
    "from structure.transformer import Transformer\n",
    "from structure.Dataset import English_Hindi_Dataset\n",
    "\n",
    "from sub_modules.embedding import Language_Embedding\n",
    "from sub_modules.masks import get_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "read_max = 7_00_000 ######\n",
    "\n",
    "# basics\n",
    "batch_size = 512\n",
    "sequence_length = 100\n",
    "d_model = 512\n",
    "num_of_sentences = 3_00_000\n",
    "# transfomer\n",
    "num_encoder_decoder_layers = 6\n",
    "num_heads = 8\n",
    "hidden_layers = 2048\n",
    "\n",
    "dropout_ff = 0.3\n",
    "dropout_attn = 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique characters: English-> 97 Hindi-> 174\n",
      "\tDataset Cleaned\n",
      "\tDataset Tokenized and Pading is Done\n"
     ]
    }
   ],
   "source": [
    "dataset = English_Hindi_Dataset('Dataset/train.en/train.en', \n",
    "                                    'Dataset/train.hi/train.hi',\n",
    "                                    num_of_sentences = num_of_sentences,\n",
    "                                    max_sequence_length = sequence_length,\n",
    "                                    read_max = read_max)\n",
    "\n",
    "en_vocab_size = len(set(dataset.en_vocab))\n",
    "hi_vocab_size = len(set(dataset.hi_vocab))\n",
    "\n",
    "assert len(dataset) == num_of_sentences, f\"Dataset is of length: {len(dataset)} but required sample :{num_of_sentences}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "embeddings = Language_Embedding(en_vocab_size, hi_vocab_size, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - int(0.8 * dataset_size)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size,val_size])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initializations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using: cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU for training\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\" Using: {device}\")\n",
    "\n",
    "model = Transformer(\n",
    "    num_encoder_decoder_layers=num_encoder_decoder_layers,\n",
    "    d_model=d_model,\n",
    "    sequence_length=sequence_length,\n",
    "    hidden_layers=hidden_layers,\n",
    "    num_heads=num_heads,\n",
    "    hi_voab_size=hi_vocab_size,\n",
    "    dropout_ff=dropout_ff,\n",
    "    dropout_attn=dropout_attn\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Loss\n",
    "criterian = nn.CrossEntropyLoss(ignore_index= dataset.hindi_to_index[dataset.PADDING_TOKEN], reduction ='none')\n",
    "\n",
    "# Parameter Initialization\n",
    "for param in model.parameters():\n",
    "    if param.dim()>1:\n",
    "        nn.init.xavier_uniform_(param)\n",
    "        \n",
    "# optimizer \n",
    "optim = torch.optim.Adam(model.parameters(), lr= 1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved model found. Training from scratch.\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"saved_models\"  # Specify your directory to save models\n",
    "os.makedirs(model_save_path, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "\n",
    "def get_latest_model_checkpoint(model_save_path):\n",
    "    model_files = os.listdir(model_save_path)\n",
    "    model_epochs = [int(re.findall(r'model_epoch_(\\d+).pt', file)[0]) for file in model_files if file.endswith('.pt')]\n",
    "    \n",
    "    if len(model_epochs)>0:\n",
    "        latest_epoch = max(model_epochs)\n",
    "        model_save_file = os.path.join(model_save_path, f\"model_epoch_{latest_epoch}.pt\")\n",
    "        return latest_epoch, model_save_file\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "latest_epoch, model_save_file = get_latest_model_checkpoint(model_save_path)\n",
    "\n",
    "if model_save_file:\n",
    "    print(f\"Loading model from {model_save_file}\")\n",
    "    model.load_state_dict(torch.load(model_save_file))\n",
    "    current_epoch = latest_epoch + 1\n",
    "else:\n",
    "    print(\"No saved model found. Training from scratch.\")\n",
    "    current_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "total_epochs = 100\n",
    "\n",
    "for epoch in range(current_epoch, total_epochs + 1):\n",
    "    print(f\"Epoch -> {epoch}\")\n",
    "    total_loss = 0\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    train_data_iterator = iter(train_data_loader)\n",
    "\n",
    "    for batch_num, batch in enumerate(tqdm(train_data_iterator, desc=f'Epoch {epoch}/{total_epochs}', unit='batch')):\n",
    "        model.train()\n",
    "        en_batch, hi_batch = batch\n",
    "        en_batch = en_batch.to(device)\n",
    "        hi_batch = hi_batch.to(device)\n",
    "\n",
    "        ds_mask, es_mask, edc_mask = get_masks(dataset, en_batch, hi_batch)\n",
    "        ds_mask, es_mask, edc_mask = ds_mask.to(device), es_mask.to(device), edc_mask.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        en_batch_embedded, hi_batch_embedded = embeddings(en_batch, hi_batch)\n",
    "        en_batch_embedded, hi_batch_embedded = en_batch_embedded.to(device), hi_batch_embedded.to(device)\n",
    "        hi_prediction = model(en_batch_embedded, hi_batch_embedded, ds_mask, es_mask, edc_mask)\n",
    "\n",
    "        # Prepare labels\n",
    "        labels_untoken = [dataset.untokenize(hi_batch[index], dataset.index_to_hindi) for index in range(len(hi_batch))]\n",
    "        labels = [dataset.tokenize(labels_untoken[index], dataset.hindi_to_index, start_token=False, end_token=True) for index in range(len(hi_batch))]\n",
    "        labels = torch.stack(labels).to(device)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterian(\n",
    "            hi_prediction.view(-1, hi_vocab_size),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "\n",
    "        # Mask padding tokens\n",
    "        valid_indices = (labels.view(-1) != dataset.hindi_to_index[dataset.PADDING_TOKEN])\n",
    "        loss = loss[valid_indices].mean()  # Calculate the mean loss over valid indices\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        # Log loss periodically\n",
    "        if batch_num % 300 == 0:\n",
    "            writer.add_scalar('Loss/Batch', loss.item(), epoch * len(train_data_iterator) + batch_num)\n",
    "    \n",
    "    avg_loss = total_loss / (batch_num + 1)\n",
    "    writer.add_scalar('Loss/Epoch', avg_loss, epoch)\n",
    "    print(f\"\\t\\tEpoch [{epoch + 1}/{total_epochs}], training Loss: {avg_loss:.4f}\")\n",
    " \n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size=batch_size,shuffle=False, pin_memory=True)\n",
    "    val_data_iterator = iter(val_data_loader)\n",
    "    with torch.no_grad():\n",
    "        for val_batch_num, val_batch in enumerate(tqdm(val_data_iterator, desc=f'Validation Epoch {epoch }/{total_epochs}', unit='batch')):\n",
    "            en_val_batch, hi_val_batch = val_batch\n",
    "            en_val_batch = en_val_batch.to(device)\n",
    "            hi_val_batch = hi_val_batch.to(device)\n",
    "            \n",
    "            ds_val_mask, es_val_mask, edc_val_mask = get_masks(dataset, en_val_batch, hi_val_batch)\n",
    "            ds_val_mask, es_val_mask, edc_val_mask = ds_val_mask.to(device), es_val_mask.to(device), edc_val_mask.to(device)\n",
    "            \n",
    "            en_val_embedded, hi_val_embedded = embeddings(en_val_batch, hi_val_batch)\n",
    "            en_val_embedded, hi_val_embedded = en_val_embedded.to(device), hi_val_embedded.to(device)\n",
    "            \n",
    "            hi_val_prediction = model(en_val_embedded, hi_val_embedded, ds_val_mask, es_val_mask, edc_val_mask)\n",
    "            \n",
    "            val_labels = [dataset.untokenize(hi_val_batch[index], dataset.index_to_hindi) for index in range(len(hi_val_batch))]\n",
    "            val_labels = [dataset.tokenize(val_labels[index], dataset.hindi_to_index, start_token=False, end_token=True) for index in range(len(hi_val_batch))]\n",
    "            val_labels = torch.stack(val_labels) \n",
    "            \n",
    "            val_loss_batch = criterian(\n",
    "                hi_val_prediction.view(-1, hi_vocab_size).to(device),\n",
    "                val_labels.view(-1).to(device)\n",
    "            ).to(device)\n",
    "            \n",
    "            valid_val_indices = torch.where(val_labels.view(-1) == dataset.hindi_to_index[dataset.PADDING_TOKEN], False, True)\n",
    "            val_loss_batch = val_loss_batch.sum() / valid_val_indices.sum()\n",
    "            \n",
    "            val_loss += val_loss_batch.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / (val_batch_num + 1)  # Average validation loss for the epoch\n",
    "    writer.add_scalar('Loss/Validation_Epoch', avg_val_loss, epoch)\n",
    "\n",
    "\n",
    "    ####### Print Epoch Losses #######\n",
    "    print(f\"\\t\\tEpoch [{epoch}/{total_epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    print('\\n')\n",
    "    # Save Model\n",
    "    model_save_file = os.path.join(model_save_path, f\"model_epoch_{epoch }.pt\")\n",
    "    torch.save(model.state_dict(), model_save_file)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(en_sentence):\n",
    "    model.eval()\n",
    "    en_sentence = (en_sentence,)\n",
    "    hi_sentence = (\"\",)\n",
    "\n",
    "    en_token = dataset.tokenize(en_sentence[0], dataset.english_to_index, start_token=False, end_token=False).unsqueeze(0).to(device)\n",
    "    hi_token = dataset.tokenize(hi_sentence[0], dataset.hindi_to_index, start_token=True, end_token=False).unsqueeze(0).to(device)\n",
    "    \n",
    "    for word_counter in range(dataset.max_sequence_length):\n",
    "        # print(f\"Processing for {word_counter + 1} token\")\n",
    "    \n",
    "        ds_mask, es_mask, edc_mask = get_masks(dataset, en_token, hi_token)\n",
    "        ds_mask, es_mask, edc_mask = ds_mask.to(device), es_mask.to(device), edc_mask.to(device)\n",
    "        \n",
    "        en_embedded, hi_embedded = embeddings(en_token, hi_token)\n",
    "        en_embedded, hi_embedded =  en_embedded.to(device), hi_embedded.to(device)\n",
    "        \n",
    "        predictions = model(en_embedded,\n",
    "                            hi_embedded,\n",
    "                            ds_mask, es_mask, edc_mask)\n",
    "        next_token_prob_distribution = predictions[0][word_counter]\n",
    "        next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
    "        next_token = dataset.index_to_hindi[next_token_index]\n",
    "        \n",
    "        if next_token == dataset.END_TOKEN:\n",
    "            break\n",
    "        hi_sentence = (hi_sentence[0] + next_token, )\n",
    "        hi_token = dataset.tokenize(hi_sentence[0], dataset.hindi_to_index, start_token=True, end_token=False).unsqueeze(0).to(device)\n",
    "        # print(f\"\\t\\t\\t Predicted till now: {hi_sentence[0]}\")\n",
    "    \n",
    "    return hi_sentence[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = dataset.untokenize(dataset[0][0], dataset.index_to_english)\n",
    "hi = dataset.untokenize(dataset[0][1], dataset.index_to_hindi)\n",
    "translation = translate(en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en sentence : In reply, Pakistan got off to a solid start.\n",
      "actual translation : जिसके जवाब में पाक ने अच्छी शुरुआत की थी.\n",
      "predicted translation : जवाब में पाकिस्तान ने एक ठोस शुरुआत की है।\n"
     ]
    }
   ],
   "source": [
    "print(f\"en sentence : {en}\")\n",
    "print(f\"actual translation : {hi}\")\n",
    "print(f\"predicted translation : {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = \"I am so mad at you.\"\n",
    "line2 = \"This is a beautiful day to go out.\"\n",
    "line3 = \"India is situated on the right side of pakistan\"\n",
    "lines = [line1, line2, line3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []\n",
    "for line in lines:\n",
    "    translations.append(translate(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am so mad at you. -> मैं तुम्हारे ऊपर बहुत बड़ा प्रयोग कर रहा हूं।\n",
      "This is a beautiful day to go out. -> यह दिन बाहर जाने का बहुत खूबसूरत है।\n",
      "India is situated on the right side of pakistan -> भारत पाकिस्तान के दाहिने तरफ से स्थित है।\n"
     ]
    }
   ],
   "source": [
    "for index, (en, hi) in enumerate(zip(lines, translations)):\n",
    "    print(f\"{en} -> {hi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'आज हम खाने के लिए जा सकते हैं'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"we can go to eat out today\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save to a pickle file\n",
    "with open('dicts.pkl', 'wb') as f:\n",
    "    pickle.dump(dict, f)\n",
    "\n",
    "# To load later:\n",
    "with open('dicts.pkl', 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['en_vocab', 'hi_vocab', 'en_to_index', 'index_to_en', 'hi_to_index', 'index_to_hi'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ',\n",
       " 1: '!',\n",
       " 2: '\"',\n",
       " 3: '#',\n",
       " 4: '$',\n",
       " 5: '%',\n",
       " 6: '&',\n",
       " 7: \"'\",\n",
       " 8: '(',\n",
       " 9: ')',\n",
       " 10: '*',\n",
       " 11: '+',\n",
       " 12: ',',\n",
       " 13: '-',\n",
       " 14: '.',\n",
       " 15: '/',\n",
       " 16: '0',\n",
       " 17: '1',\n",
       " 18: '2',\n",
       " 19: '3',\n",
       " 20: '4',\n",
       " 21: '5',\n",
       " 22: '6',\n",
       " 23: '7',\n",
       " 24: '8',\n",
       " 25: '9',\n",
       " 26: ':',\n",
       " 27: '<',\n",
       " 28: '<END>',\n",
       " 29: '<PADDING>',\n",
       " 30: '<START>',\n",
       " 31: '=',\n",
       " 32: '>',\n",
       " 33: '?',\n",
       " 34: '@',\n",
       " 35: '[',\n",
       " 36: '\\\\',\n",
       " 37: ']',\n",
       " 38: '^',\n",
       " 39: '_',\n",
       " 40: '`',\n",
       " 41: '{',\n",
       " 42: '|',\n",
       " 43: '}',\n",
       " 44: '~',\n",
       " 45: 'ऀ',\n",
       " 46: 'ँ',\n",
       " 47: 'ं',\n",
       " 48: 'ः',\n",
       " 49: 'ऄ',\n",
       " 50: 'अ',\n",
       " 51: 'आ',\n",
       " 52: 'इ',\n",
       " 53: 'ई',\n",
       " 54: 'उ',\n",
       " 55: 'ऊ',\n",
       " 56: 'ऋ',\n",
       " 57: 'ऌ',\n",
       " 58: 'ऍ',\n",
       " 59: 'ऎ',\n",
       " 60: 'ए',\n",
       " 61: 'ऐ',\n",
       " 62: 'ऑ',\n",
       " 63: 'ऒ',\n",
       " 64: 'ओ',\n",
       " 65: 'औ',\n",
       " 66: 'क',\n",
       " 67: 'ख',\n",
       " 68: 'ग',\n",
       " 69: 'घ',\n",
       " 70: 'ङ',\n",
       " 71: 'च',\n",
       " 72: 'छ',\n",
       " 73: 'ज',\n",
       " 74: 'झ',\n",
       " 75: 'ञ',\n",
       " 76: 'ट',\n",
       " 77: 'ठ',\n",
       " 78: 'ड',\n",
       " 79: 'ढ',\n",
       " 80: 'ण',\n",
       " 81: 'त',\n",
       " 82: 'थ',\n",
       " 83: 'द',\n",
       " 84: 'ध',\n",
       " 85: 'न',\n",
       " 86: 'ऩ',\n",
       " 87: 'प',\n",
       " 88: 'फ',\n",
       " 89: 'ब',\n",
       " 90: 'भ',\n",
       " 91: 'म',\n",
       " 92: 'य',\n",
       " 93: 'र',\n",
       " 94: 'ऱ',\n",
       " 95: 'ल',\n",
       " 96: 'ळ',\n",
       " 97: 'ऴ',\n",
       " 98: 'व',\n",
       " 99: 'श',\n",
       " 100: 'ष',\n",
       " 101: 'स',\n",
       " 102: 'ह',\n",
       " 103: 'ऺ',\n",
       " 104: 'ऻ',\n",
       " 105: '़',\n",
       " 106: 'ऽ',\n",
       " 107: 'ा',\n",
       " 108: 'ि',\n",
       " 109: 'ी',\n",
       " 110: 'ु',\n",
       " 111: 'ू',\n",
       " 112: 'ृ',\n",
       " 113: 'ॄ',\n",
       " 114: 'ॅ',\n",
       " 115: 'ॆ',\n",
       " 116: 'े',\n",
       " 117: 'ै',\n",
       " 118: 'ॉ',\n",
       " 119: 'ॊ',\n",
       " 120: 'ो',\n",
       " 121: 'ौ',\n",
       " 122: '्',\n",
       " 123: 'ॎ',\n",
       " 124: 'ॏ',\n",
       " 125: 'ॐ',\n",
       " 126: '॑',\n",
       " 127: '॒',\n",
       " 128: '॓',\n",
       " 129: '॔',\n",
       " 130: 'ॕ',\n",
       " 131: 'ॖ',\n",
       " 132: 'ॗ',\n",
       " 133: 'क़',\n",
       " 134: 'ख़',\n",
       " 135: 'ग़',\n",
       " 136: 'ज़',\n",
       " 137: 'ड़',\n",
       " 138: 'ढ़',\n",
       " 139: 'फ़',\n",
       " 140: 'य़',\n",
       " 141: 'ॠ',\n",
       " 142: 'ॡ',\n",
       " 143: 'ॢ',\n",
       " 144: 'ॣ',\n",
       " 145: '।',\n",
       " 146: '॥',\n",
       " 147: '०',\n",
       " 148: '१',\n",
       " 149: '२',\n",
       " 150: '३',\n",
       " 151: '४',\n",
       " 152: '५',\n",
       " 153: '६',\n",
       " 154: '७',\n",
       " 155: '८',\n",
       " 156: '९',\n",
       " 157: '॰',\n",
       " 158: 'ॱ',\n",
       " 159: 'ॲ',\n",
       " 160: 'ॳ',\n",
       " 161: 'ॴ',\n",
       " 162: 'ॵ',\n",
       " 163: 'ॶ',\n",
       " 164: 'ॷ',\n",
       " 165: 'ॸ',\n",
       " 166: 'ॹ',\n",
       " 167: 'ॺ',\n",
       " 168: 'ॻ',\n",
       " 169: 'ॼ',\n",
       " 170: 'ॽ',\n",
       " 171: 'ॾ',\n",
       " 172: '“',\n",
       " 173: '”'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dict['index_to_hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def translate(en_sentence):\n",
    "    model.eval()\n",
    "    en_sentence = (en_sentence,)\n",
    "    hi_sentence = (\"\",)\n",
    "\n",
    "    en_token = dataset.tokenize(en_sentence[0], dataset.english_to_index, start_token=False, end_token=False).unsqueeze(0).to(device)\n",
    "    hi_token = dataset.tokenize(hi_sentence[0], dataset.hindi_to_index, start_token=True, end_token=False).unsqueeze(0).to(device)\n",
    "    \n",
    "    print(en_token)\n",
    "    print(hi_token)\n",
    "    for word_counter in range(dataset.max_sequence_length):\n",
    "        # print(f\"Processing for {word_counter + 1} token\")\n",
    "    \n",
    "        ds_mask, es_mask, edc_mask = get_masks(dataset, en_token, hi_token)\n",
    "        ds_mask, es_mask, edc_mask = ds_mask.to(device), es_mask.to(device), edc_mask.to(device)\n",
    "        \n",
    "        en_embedded, hi_embedded = embeddings(en_token, hi_token)\n",
    "        en_embedded, hi_embedded =  en_embedded.to(device), hi_embedded.to(device)\n",
    "        \n",
    "        predictions = model(en_embedded,\n",
    "                            hi_embedded,\n",
    "                            ds_mask, es_mask, edc_mask)\n",
    "        next_token_prob_distribution = predictions[0][word_counter]\n",
    "        next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
    "        next_token = dataset.index_to_hindi[next_token_index]\n",
    "        \n",
    "        if next_token == dataset.END_TOKEN:\n",
    "            break\n",
    "        hi_sentence = (hi_sentence[0] + next_token, )\n",
    "        hi_token = dataset.tokenize(hi_sentence[0], dataset.hindi_to_index, start_token=True, end_token=False).unsqueeze(0).to(device)\n",
    "        # print(f\"\\t\\t\\t Predicted till now: {hi_sentence[0]}\")\n",
    "    \n",
    "    return hi_sentence[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[42, 71, 78, 78, 81, 12,  0, 42, 81, 89,  0, 67, 84, 71,  0, 91, 81, 87,\n",
      "         33, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29]], device='cuda:0')\n",
      "tensor([[30, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "         29, 29, 29, 29, 29, 29, 29, 29, 29, 29]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'हैलो, तुम कैसे हो?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line1 = \"Hello, How are you?\"\n",
    "translate(line1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dictionaries\n",
    "import json\n",
    "\n",
    "with open('dicts.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "data.keys()\n",
    "\n",
    "en_vocab = data['en_vocab']\n",
    "hi_vocab = data['hi_vocab']\n",
    "en_to_index = data['en_to_index']\n",
    "index_to_en = data['index_to_en']\n",
    "hi_to_index = data['hi_to_index']\n",
    "index_to_hi = data['index_to_hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': ' ',\n",
       " '1': '!',\n",
       " '2': '\"',\n",
       " '3': '#',\n",
       " '4': '$',\n",
       " '5': '%',\n",
       " '6': '&',\n",
       " '7': \"'\",\n",
       " '8': '(',\n",
       " '9': ')',\n",
       " '10': '*',\n",
       " '11': '+',\n",
       " '12': ',',\n",
       " '13': '-',\n",
       " '14': '.',\n",
       " '15': '/',\n",
       " '16': '0',\n",
       " '17': '1',\n",
       " '18': '2',\n",
       " '19': '3',\n",
       " '20': '4',\n",
       " '21': '5',\n",
       " '22': '6',\n",
       " '23': '7',\n",
       " '24': '8',\n",
       " '25': '9',\n",
       " '26': ':',\n",
       " '27': '<',\n",
       " '28': '<END>',\n",
       " '29': '<PADDING>',\n",
       " '30': '<START>',\n",
       " '31': '=',\n",
       " '32': '>',\n",
       " '33': '?',\n",
       " '34': '@',\n",
       " '35': '[',\n",
       " '36': '\\\\',\n",
       " '37': ']',\n",
       " '38': '^',\n",
       " '39': '_',\n",
       " '40': '`',\n",
       " '41': '{',\n",
       " '42': '|',\n",
       " '43': '}',\n",
       " '44': '~',\n",
       " '45': 'ऀ',\n",
       " '46': 'ँ',\n",
       " '47': 'ं',\n",
       " '48': 'ः',\n",
       " '49': 'ऄ',\n",
       " '50': 'अ',\n",
       " '51': 'आ',\n",
       " '52': 'इ',\n",
       " '53': 'ई',\n",
       " '54': 'उ',\n",
       " '55': 'ऊ',\n",
       " '56': 'ऋ',\n",
       " '57': 'ऌ',\n",
       " '58': 'ऍ',\n",
       " '59': 'ऎ',\n",
       " '60': 'ए',\n",
       " '61': 'ऐ',\n",
       " '62': 'ऑ',\n",
       " '63': 'ऒ',\n",
       " '64': 'ओ',\n",
       " '65': 'औ',\n",
       " '66': 'क',\n",
       " '67': 'ख',\n",
       " '68': 'ग',\n",
       " '69': 'घ',\n",
       " '70': 'ङ',\n",
       " '71': 'च',\n",
       " '72': 'छ',\n",
       " '73': 'ज',\n",
       " '74': 'झ',\n",
       " '75': 'ञ',\n",
       " '76': 'ट',\n",
       " '77': 'ठ',\n",
       " '78': 'ड',\n",
       " '79': 'ढ',\n",
       " '80': 'ण',\n",
       " '81': 'त',\n",
       " '82': 'थ',\n",
       " '83': 'द',\n",
       " '84': 'ध',\n",
       " '85': 'न',\n",
       " '86': 'ऩ',\n",
       " '87': 'प',\n",
       " '88': 'फ',\n",
       " '89': 'ब',\n",
       " '90': 'भ',\n",
       " '91': 'म',\n",
       " '92': 'य',\n",
       " '93': 'र',\n",
       " '94': 'ऱ',\n",
       " '95': 'ल',\n",
       " '96': 'ळ',\n",
       " '97': 'ऴ',\n",
       " '98': 'व',\n",
       " '99': 'श',\n",
       " '100': 'ष',\n",
       " '101': 'स',\n",
       " '102': 'ह',\n",
       " '103': 'ऺ',\n",
       " '104': 'ऻ',\n",
       " '105': '़',\n",
       " '106': 'ऽ',\n",
       " '107': 'ा',\n",
       " '108': 'ि',\n",
       " '109': 'ी',\n",
       " '110': 'ु',\n",
       " '111': 'ू',\n",
       " '112': 'ृ',\n",
       " '113': 'ॄ',\n",
       " '114': 'ॅ',\n",
       " '115': 'ॆ',\n",
       " '116': 'े',\n",
       " '117': 'ै',\n",
       " '118': 'ॉ',\n",
       " '119': 'ॊ',\n",
       " '120': 'ो',\n",
       " '121': 'ौ',\n",
       " '122': '्',\n",
       " '123': 'ॎ',\n",
       " '124': 'ॏ',\n",
       " '125': 'ॐ',\n",
       " '126': '॑',\n",
       " '127': '॒',\n",
       " '128': '॓',\n",
       " '129': '॔',\n",
       " '130': 'ॕ',\n",
       " '131': 'ॖ',\n",
       " '132': 'ॗ',\n",
       " '133': 'क़',\n",
       " '134': 'ख़',\n",
       " '135': 'ग़',\n",
       " '136': 'ज़',\n",
       " '137': 'ड़',\n",
       " '138': 'ढ़',\n",
       " '139': 'फ़',\n",
       " '140': 'य़',\n",
       " '141': 'ॠ',\n",
       " '142': 'ॡ',\n",
       " '143': 'ॢ',\n",
       " '144': 'ॣ',\n",
       " '145': '।',\n",
       " '146': '॥',\n",
       " '147': '०',\n",
       " '148': '१',\n",
       " '149': '२',\n",
       " '150': '३',\n",
       " '151': '४',\n",
       " '152': '५',\n",
       " '153': '६',\n",
       " '154': '७',\n",
       " '155': '८',\n",
       " '156': '९',\n",
       " '157': '॰',\n",
       " '158': 'ॱ',\n",
       " '159': 'ॲ',\n",
       " '160': 'ॳ',\n",
       " '161': 'ॴ',\n",
       " '162': 'ॵ',\n",
       " '163': 'ॶ',\n",
       " '164': 'ॷ',\n",
       " '165': 'ॸ',\n",
       " '166': 'ॹ',\n",
       " '167': 'ॺ',\n",
       " '168': 'ॻ',\n",
       " '169': 'ॼ',\n",
       " '170': 'ॽ',\n",
       " '171': 'ॾ',\n",
       " '172': '“',\n",
       " '173': '”'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
