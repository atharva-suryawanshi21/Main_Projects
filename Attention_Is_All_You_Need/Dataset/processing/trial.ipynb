{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en_path = \"../train.en/train.en\"\n",
    "train_hi_path = \"../train.hi/train.hi\"\n",
    "\n",
    "with open(train_en_path, 'r') as file:\n",
    "    data_en = file.readlines()\n",
    "\n",
    "with open(train_hi_path, 'r') as file:\n",
    "    data_hi = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8568307, 8568307)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_en), len(data_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 174)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_TOKEN = '<START>'\n",
    "PADDING_TOKEN = '<PADDING>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "english_vocabulary = [\n",
    "    START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', \n",
    "    ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', '@', \n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', \n",
    "    'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', \n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', \n",
    "    'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN\n",
    "]\n",
    "\n",
    "\n",
    "hindi_vocabulary = [\n",
    "    START_TOKEN, PADDING_TOKEN, END_TOKEN, '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "    # Adding Devanagari characters (vowels and consonants)\n",
    "    *(chr(code) for code in range(0x0900, 0x097F)),\n",
    "    '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', '<', '=', '>', '?', '@', \n",
    "    '[', '\\\\', ']', '^', '_', '`', '।', '“', '”', '{', '|', '}', '~'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# for char in hindi_vocabulary:\n",
    "#     print(char.encode('utf-8').decode('utf-8'))\n",
    "len(english_vocabulary), len(hindi_vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 173)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(english_vocabulary)), len(set(hindi_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocabulary = list(set(english_vocabulary))\n",
    "hindi_vocabulary = list(set(hindi_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_hindi = {k:v for k,v in enumerate(hindi_vocabulary)}\n",
    "hindi_to_index = {v:k for k,v in enumerate(hindi_vocabulary)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
    "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Sentences = 500\n",
    "\n",
    "data_en, data_hi = data_en[:Total_Sentences], data_hi[:Total_Sentences]\n",
    "data_en = [sentence.rstrip() for sentence in data_en]\n",
    "data_hi = [sentence.rstrip() for sentence in data_hi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492, 576)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(x) for x in data_en), max(len(x) for x in data_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_length(max_sequence_length, en, hi):\n",
    "    return len(en)<= max_sequence_length - 2  and len(hi)<= max_sequence_length - 2  # we may add start token and end token\n",
    "\n",
    "def valid_vocab(en, hi, english_vocabulary, hindi_vocabulary):\n",
    "    hindi_vocabulary = set(hindi_vocabulary)\n",
    "    english_vocabulary = set(english_vocabulary)\n",
    "    \n",
    "    for chr in en:\n",
    "        if chr not in english_vocabulary:\n",
    "            return False\n",
    "        \n",
    "    for chr in hi:\n",
    "        if chr not in hindi_vocabulary:\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = 200\n",
    "final_sentences = 20\n",
    "\n",
    "en_sentences = []\n",
    "hi_sentences = []\n",
    "total = 0\n",
    "bad_ones = 0\n",
    "\n",
    "for index, (en, hi) in enumerate(zip(data_en, data_hi)):\n",
    "    if valid_length(max_sequence_length, en, hi) and valid_vocab(en, hi, english_vocabulary, hindi_vocabulary):\n",
    "        en_sentences.append(en)\n",
    "        hi_sentences.append(hi)\n",
    "        total += 1\n",
    "    else:\n",
    "        bad_ones += 1        \n",
    "\n",
    "    if total == final_sentences:\n",
    "        break\n",
    "    \n",
    "len(hi_sentences), len(en_sentences), bad_ones\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173, 189)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(x) for x in en_sentences), max(len(x) for x in hi_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "nil = -1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]])\n"
     ]
    }
   ],
   "source": [
    "nil = -1e9\n",
    "\n",
    "encoder_self_attention_mask = torch.full([3, 5, 5], 0.0)\n",
    "encoder_self_attention_mask[0, 3:, : ] = nil\n",
    "encoder_self_attention_mask[0, :, 3: ] = nil\n",
    "print(encoder_self_attention_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nil = -1e9\n",
    "\n",
    "def get_masks(en_batch, hi_batch):\n",
    "    num_of_sentences = len(en_batch)\n",
    "    \n",
    "    decoder_self_attention_mask = torch.full([num_of_sentences, max_sequence_length, max_sequence_length] , nil)\n",
    "    decoder_self_attention_mask = torch.triu(decoder_self_attention_mask , diagonal = 1)\n",
    "    \n",
    "    encoder_self_attention_mask = torch.full([num_of_sentences, max_sequence_length, max_sequence_length], 0.0)\n",
    "    encoder_decoder_attention_mask = torch.full([num_of_sentences, max_sequence_length, max_sequence_length], 0.0)\n",
    "    \n",
    "    for index in range(num_of_sentences):\n",
    "        num_of_en_tokens, num_of_hi_tokens = len(en_batch[index]), len(hi_batch[index])\n",
    "        \n",
    "        encoder_self_attention_mask[index, num_of_en_tokens:, : ] = nil\n",
    "        encoder_self_attention_mask[index, :, num_of_en_tokens: ] = nil\n",
    "        \n",
    "        encoder_decoder_attention_mask[index, num_of_hi_tokens: , : ] = nil\n",
    "        encoder_decoder_attention_mask[index, : , num_of_en_tokens: ] = nil\n",
    "        \n",
    "    return decoder_self_attention_mask,encoder_self_attention_mask, encoder_decoder_attention_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09],\n",
       "        [ 0.0000e+00,  0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n",
       "         -1.0000e+09, -1.0000e+09],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "         -1.0000e+09, -1.0000e+09],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00, -1.0000e+09],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_batch = torch.rand(1,3)\n",
    "hi_batch = torch.rand(1,2)\n",
    "ds,es,eds = get_masks(en_batch, hi_batch)\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def tokenize(sentences, language_to_index, start_token=False, end_token=False):\n",
    "    sentence_indices = [language_to_index[token] for token in list(sentences)]\n",
    "    \n",
    "    if start_token:\n",
    "        sentence_indices.insert(0, language_to_index[START_TOKEN])\n",
    "    if end_token:\n",
    "        sentence_indices.append(language_to_index[END_TOKEN])\n",
    "        \n",
    "    while len(sentence_indices) < max_sequence_length:\n",
    "        sentence_indices.append(language_to_index[PADDING_TOKEN])\n",
    "            \n",
    "    return torch.tensor(sentence_indices)\n",
    "\n",
    "def get_tokenized_sentences(sentences, language_to_index, start_token=False, end_token=False):\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        yield tokenize(sentence, language_to_index, start_token, end_token)\n",
    "    \n",
    "en_tokenized = list(get_tokenized_sentences(en_sentences, english_to_index, start_token=False, end_token=False))\n",
    "hi_tokenized = list(get_tokenized_sentences(hi_sentences, hindi_to_index, start_token=True, end_token=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('In reply, Pakistan got off to a solid start.',\n",
       " 'जिसके जवाब में पाक ने अच्छी शुरुआत की थी.')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, en_sentences, hi_sentences):\n",
    "        super().__init__()\n",
    "        self.en_sentences = en_sentences\n",
    "        self.hi_sentences = hi_sentences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.en_sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.en_sentences[index], self.hi_sentences[index]\n",
    "        \n",
    "dataset = TextDataset(en_sentences, hi_sentences)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset = dataset, batch_size= 4, shuffle=True, num_workers=4)\n",
    "iterator = iter(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
